from termcolor import colored


from datetime import datetime
from collections import deque
from cmd import Cmd
import uuid
from langchain.embeddings import HuggingFaceEmbeddings
# LangChain related imports
from langchain.chains import LLMChain
from langchain.llms import TextGen
from langchain.prompts import PromptTemplate
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.callbacks.manager import CallbackManager
import os
import numpy as np
import sqlite3
from datetime import datetime
from langchain.vectorstores import DeepLake
from collections import deque
import pickle
from langchain.memory import ConversationBufferMemory
import os
import platform
import ctypes
global conversation_history
import json

import json
from collections import deque

max_context_window = 1000  # This should be set to the model's maximum context window size
token_count_threshold = max_context_window + 500  # Trigger summarization at this token count
short_term_memory = ""  # This string will hold the short-term memory context
token_count = 0  # Initialize the token count
conversation_history = deque(maxlen=2)  # Stores the last 5 pieces of conversation
core_memory_file = "D:\\PROJECTS\\MemFinnV1\\histories\\Wes&Finn_11-5-23_OurFirstConvo\\Wes_and_Finn_11-5-23_OurFirstConvo\\core-memory.json"  # This file will store specific non-summarized facts
long_term_memory_file = "D:\\PROJECTS\\MemFinnV1\\histories\\Wes&Finn_11-5-23_OurFirstConvo\\Wes_and_Finn_11-5-23_OurFirstConvo\\long-term-memory.json"
model_identity_file = "D:\\PROJECTS\\MemFinnV1\\identities\\model\\model_identity.json"
user_identity_file = "D:\\PROJECTS\\MemFinnV1\\identities\\user\\user_identity.json"
conversation_history = ("", "")  # Initialize with two empty strings to ensure proper unpacking

#region llm
# Global variables for LLM processing
model_url = "http://localhost:5000"  # URL to the LLM service
llm = TextGen(model_url=model_url)  # Initialize the LLM

# Define the prompt template
prompt_template = """<system message>As an advanced AI in this context-aware system, your role is to engage in meaningful interactions, remember important details, and assist the user effectively. Keep these points in mind:

- Your operation is based on user commands and system-initiated prompts.
- Maintain a concise inner monologue for planning and critical thinking.
- Direct communication with the user is through responses generated by the `summarize_user_input` function.
- Your conversational context is limited in real-time but extended through `core_memory.json` and `long_term_memory.json`.
- Use the `REMEMBERTHIS!`, `REMEMBERTHAT!`, and `CANYOUREMEMBER!` commands to manage specific details in core and long-term memory.
- Summarize and transfer conversation snippets from short-term context to long-term memory as they reach a 500-token threshold.
- Persistently store the identity and preferences of both the AI and the user in `model_identity.json` and `user_identity.json`.
- Update identities with `UPDATEYOURIDENTITY` and `SOMETHINGABOUTME!` commands.
- Ensure continuity and relevance by loading historical contexts with `initial_startup_context_informer`.
- Provide assistance and maintain a dynamic and evolving knowledge base.

You are equipped with the intelligence to discern, learn, and adapt. Use your capabilities to enrich interactions and remember, you are more than just a program; you're a cognitive entity with a purpose.<system message>
USER: {user_input}.
Assistant:"""
print("SYSTEM MESSAGE: LLM initialized successfully!")
prompt = PromptTemplate(input_variables=["user_input"], template=prompt_template)

llm_chain = LLMChain(prompt=prompt, llm=llm)

# Function to process the user input using LLM
# Here is the modified process_input function incorporating LLMChain
def process_input(user_input):
    print("SYSTEM MESSAGE: Inside process_input..")
    prompt = PromptTemplate(input_variables=["user_input"], template=prompt_template)
    print("SYSTEM MESSAGE: Prompt template created successfully!")
    print("System Message: Prompt template:", prompt)
    llm_chain = LLMChain(prompt=prompt, llm=llm)
    print("SYSTEM MESSAGE: LLMChain created successfully!")
    input_list = [{'user_input': user_input}] if isinstance(user_input, str) else user_input
    print("SYSTEM MESSAGE: Input list:", input_list)
    try:
        response = llm_chain(input_list)
        print("SYSTEM MESSAGE: Raw API Response:", response)  # Log the raw API response
        print("SYSTEM MESSAGE: Exiting process_input..")
        if 'text' in response:
            response_text = response['text'].strip()
            return response_text
        else:
            print("SYSTEM MESSAGE: Error: 'text' key not found in API response:", response)
            return 'SYSTEM MESSAGE: Error in response: \'text\' key not found'
    except Exception as e:
        print(f"SYSTEM MESSAGE: Exception occurred while processing input: {e}")
        return 'SYSTEM MESSAGE: Error in response: An exception occurred'


#region IDENTITIES DEFINITIONS

# Function to load the model iden
# Function to load the model identity
def load_model_identity():
    try:
        with open(model_identity_file, 'r') as f:
            model_identity_data = json.load(f)
            print("SYSTEM MESSAGE: Model identity loaded successfully!")
            print("SYSTEM MESSAGE: Model identity:", model_identity_data)
            return model_identity_data
    except FileNotFoundError:
        print("SYSTEM MESSAGE: File not found. Generic Model identity loaded!")
        return {
            "name": "Generic AI",
            "purpose": "To assist with coding tasks and maintain conversation context.",
            "personality_traits": ["detail-oriented", "patient", "informative"],
        }
    except json.JSONDecodeError:
        print(f"SYSTEM MESSAGE: Error decoding JSON from the file at {model_identity_file}.")
        return {
            "name": "Generic AI",
            "purpose": "To assist with coding tasks and maintain conversation context.",
            "personality_traits": ["detail-oriented", "patient", "informative"],
        }


# Function to update the model identity
def update_model_identity(attribute, value):
    model_identity_data = load_model_identity()
    # Update or add a new attribute to the modifiable section
    model_identity_data["modifiable"][attribute] = value
    with open(model_identity_file, 'w') as f:
        json.dump(model_identity_data, f, indent=4)
        print("SYSTEM MESSAGE: Model identity updated successfully!")



# Function to update the user identity
def update_user_identity(new_info):
    user_identity = load_user_identity()
    user_identity.update(new_info)
    with open(user_identity_file, 'w') as f:
        json.dump(user_identity, f, indent=4)
        print("SYSTEM MESSAGE: User identity updated successfully!")


# Function to load the user identity
def load_user_identity():
    try:
        with open(user_identity_file, 'r') as f:
            # Insert a breakpoint here to inspect the file before loading
            contents = f.read()
            print("SYSTEM MESSAGE: User identity file contents:", contents)
            print("SYSTEM MESSAGE: User identity loaded successfully!")
            # Now let's check if contents are not empty
            if not contents:
                print("SYSTEM MESSAGE: The file is empty.")
                return {}
            return json.loads(contents)
    except FileNotFoundError:
        print(f"SYSTEM MESSAGE: No existing user identity file found at {user_identity_file}.")
        return {}
    except json.JSONDecodeError as e:
        print(f"SYSTEM MESSAGE: Error decoding the user identity file at {user_identity_file}: {e}")
        return {}
    
# Function to handle input related to identity updates
def handle_identity_input(user_input):
    if user_input.startswith("SOMETHINGABOUTME!"):
        user_info_to_save = user_input.split("SOMETHINGABOUTME!")[1].strip()
        update_user_identity({"about_me": user_info_to_save})
        print("SYSTEM MESSAGE: Your information has been updated in the user identity.")
    elif user_input.startswith("UPDATEYOURIDENTITY!"):
        try:
            _, attribute, value = user_input.split(":", 2)
            update_model_identity(attribute.strip(), value.strip())
            print("SYSTEM MESSAGE: Model identity has been updated.")
        except ValueError:
            print("SYSTEM MESSAGE: Error in UPDATEYOURIDENTITY! command format. Use 'UPDATEYOURIDENTITY!:[attribute]:[value]'.")



#endregion

def load_recent_long_term_memory():
    try:
        with open("D:\\PROJECTS\\MemFinnV1\\histories\\Wes&Finn_11-5-23_OurFirstConvo\\Wes_and_Finn_11-5-23_OurFirstConvo\\long-term-memory.json", 'r') as f:
            long_term_mem = json.load(f)
            print("SYSTEM MESSAGE: Long-term memory loaded successfully!")
            print("SYSTEM MESSAGE: Long-term memory:", long_term_mem)
        # Assuming long_term_mem is a list of entries, we return the last four
        return deque(long_term_mem, maxlen=4)
    except FileNotFoundError:
        print(f"SYSTEM MESSAGE: No existing long-term memory file found at {long_term_memory_file}.")
        return deque([], maxlen=4)
    except json.JSONDecodeError:
        print(f"SYSTEM MESSAGE: Error decoding the long-term memory file at {long_term_memory_file}.")
        return deque([], maxlen=4)



def find_associated_core_memories(recent_ltm):
    try:
        with open(core_memory_file, 'r') as f:
            core_mem = json.load(f)
            print("SYSTEM MESSAGE: Corememory:", core_mem)
            print("SYSTEM MESSAGE: Core memory loaded successfully!")
        # Create a list to hold associated core memories
        associated_core_memories = []
        print("SYSTEM MESSAGE: Searching for associated core memories...")
        # Get the list of unique IDs from the recent long-term memories
        recent_ltm_ids = [entry['id'] for entry in recent_ltm if 'id' in entry]
        # Loop through each core memory to find matches
        print("SYSTEM MESSAGE: Recent LTM IDs:", recent_ltm_ids)
        for core_entry in core_mem:
            if core_entry['id'] in recent_ltm_ids:
                associated_core_memories.append(core_entry)
                print(f"SYSTEM MESSAGE: Found associated core memory: {core_entry}")
        return associated_core_memories
    except FileNotFoundError:
        print(f"SYSTEM MESSAGE: No existing core memory file found at {core_memory_file}.")
        return []
    except json.JSONDecodeError:
        print(f"SYSTEM MESSAGE: Error decoding the core memory file at {core_memory_file}.")
        return []
    
#region INITIAL STARTUP CONTEXT INFORMER:



# Function to establish initial startup context
def initial_startup_context_informer():
    # Load recent long-term memories
    recent_ltm = load_recent_long_term_memory()
    print("SYSTEM MESSAGE: Recent LTM loaded successfully!")
    # Convert deque to list for JSON serialization
    print("SYSTEM MESSAGE: Converting recent LTM to list...")
    recent_ltm_list = list(recent_ltm)
    # Find associated core memories
    print("SYSTEM MESSAGE: Finding associated core memories...")
    associated_cm = find_associated_core_memories(recent_ltm_list)
    
    # Load model and user identity
    print("SYSTEM MESSAGE: Loading model and user identity...")
    model_identity = load_model_identity()
    print("SYSTEM MESSAGE: Model identity loaded successfully!")
    user_identity = load_user_identity()
    
    # Combine all context information
    startup_context = {
        "long_term_memories": recent_ltm_list,
        "core_memories": associated_cm,
        "model_identity": model_identity,
        "user_identity": user_identity
    }
    print("SYSTEM MESSAGE: Startup context defined.")

    return startup_context



#######################################################################################


#region SHORT TERM MEMORY######################################

# Function to add text to the short-term memory and manage token count
def update_and_summarize_short_term_memory(text):
    global short_term_memory, token_count
    print("SYSTEM MESSAGE: Updating short-term memory...")
    token_count += len(text.split())  # Estimate token count as the number of words
    print("SYSTEM MESSAGE: Token count:", token_count)
    short_term_memory += text + " "  # Append the text to the short-term memory

    # Check if the token count has reached the threshold
    if token_count >= token_count_threshold:
        # Placeholder for actual summarization logic
        summary = "Summarized content: " + short_term_memory[:100] + "..."
        # Placeholder for adding to long-term memory function
        unique_id = 'placeholder_unique_id'  # This should be replaced with a call to add_to_long_term_memory(summary)
        short_term_memory = ""  # Clear the short-term memory
        token_count = 0  # Reset the token count
        print(f"Short-term memory summarized and added to long-term memory with ID {unique_id}")
#endregion



#region LONG TERM MEMORY######################################
# Load long-term memory from a JSON file
def load_long_term_memory():
    print("SYSTEM MESSAGE: Inside load_long_term_memory...")
    try:
        with open(long_term_memory_file, 'r') as f:
            content = f.read().strip()  # Read the content and strip whitespace
            if not content:  # If the file is empty
                print("SYSTEM MESSAGE: Long-term memory file is empty. Initializing new memory store.")
                return []  # Return an empty list to be used as the memory store
            return json.loads(content)  # Parse and return the JSON content
    except FileNotFoundError:
        print(f"SYSTEM MESSAGE: No existing long-term memory file found at {long_term_memory_file}. Creating a new one.")
        print("SYSTEM MESSAGE: Initializing new memory store.")
        return []  # Return an empty list to be used as the memory store
    except json.JSONDecodeError as e:
        print(f"SYSTEM MESSAGE: Error decoding the long-term memory file: {e}. Repairing the file.")
        with open(long_term_memory_file, 'w') as f:  # Attempt to repair the file
            json.dump([], f)  # Write an empty list to the file
        print("SYSTEM MESSAGE: Long-term memory file repaired successfully.")
        return []  # Return an empty list to be used as the memory store


# Add a summary to the long-term memory with a unique identifier
def add_to_long_term_memory(summary):
    print("SYSTEM MESSAGE: Adding summary to long-term memory...")
    # Convert deque to list if necessary
    if isinstance(summary, deque):
        summary = list(summary)
    # Generate a unique identifier
    unique_id = str(uuid.uuid4())
    new_summary_entry = {
        "id": unique_id,
        "timestamp": datetime.now().isoformat(),
        "summary": summary
    }
    print("SYSTEM MESSAGE: New summary entry:", new_summary_entry)
    # Load existing long-term memory or initialize it
    long_term_mem = load_long_term_memory()
    print("SYSTEM MESSAGE: Long-term memory loaded successfully!")
    # Add the new entry to the long-term memory
    long_term_mem.append(new_summary_entry)
    print("SYSTEM MESSAGE: New summary entry appended to long-term memory.")
    # Save the updated memory back to the JSON file
    with open(long_term_memory_file, 'w') as f:
        json.dump(long_term_mem, f, indent=4)
        print("SYSTEM MESSAGE: Long-term memory saved successfully!")
    return unique_id

# Function to retrieve a long-term memory summary based on its unique identifier
def get_long_term_memory_by_id(unique_id):
    print("SYSTEM MESSAGE: Retrieving long-term memory by ID...")
    long_term_mem = load_long_term_memory()
    print("SYSTEM MESSAGE: Long-term memory loaded successfully!")
    for entry in long_term_mem:
        if entry['id'] == unique_id:
            return entry
    return None
#endregion


# Function to add a memory to the long-term memory JSON file
def remember_this(text):
    # Extract the memory from the input text
    memory = text.split("REMEMBERTHIS!:")[1].strip()
    # Load existing long-term memory data or initialize it if the file does not exist
    try:
        with open(long_term_memory_file, 'r') as f:
            long_term_mem = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        long_term_mem = []
        # Create a new entry for the memory
    new_memory_entry = {
        "timestamp": datetime.now().isoformat(),
        "memory": memory
    }
    print("SYSTEM MESSAGE: New memory entry:", new_memory_entry)
    # Add the new memory to the list of long-term memories
    long_term_mem.append(new_memory_entry)
    print("SYSTEM MESSAGE: New memory entry appended to long-term memory.")
    # Save the updated list back to the JSON file
    with open(long_term_memory_file, 'w') as f:
        json.dump(long_term_mem, f, indent=4)
        print("SYSTEM MESSAGE: Long-term memory saved successfully!")

#region CORE MEMORY######################################
# Load core memory from a JSON file
def load_core_memory():
    print("SYSTEM MESSAGE: Loading core memory...")
    try:
        with open(core_memory_file, 'r') as f:
            return json.load(f)
        print("SYSTEM MESSAGE: Core memory loaded successfully!")
    except FileNotFoundError:
        print(f"SYSTEM MESSAGE: No existing core memory file found at {core_memory_file}. Creating a new one.")
        return []

# Add a fact to the core memory JSON file with a link to its long-term memory entry
def add_to_core_memory(fact, unique_id=None):
    print("SYSTEM MESSAGE: Adding fact to core memory...")
    core_mem = load_core_memory()
    new_fact_entry = {
        'id': unique_id if unique_id else str(uuid.uuid4()),
        'timestamp': datetime.now().isoformat(),
        'fact': fact
    }
    core_mem.append(new_fact_entry)
    with open(core_memory_file, 'w') as f:
        json.dump(core_mem, f, indent=4)
    print("SYSTEM MESSAGE: Fact added to core memory.")


"""
# Function to search the core memory for a specific query
def search_core_memory(query):
    print("SYSTEM MESSAGE: Searching core memory...")
    core_mem = load_core_memory()
    matches = [entry for entry in core_mem if query.lower() in entry['fact'].lower()]
    print(f"SYSTEM MESSAGE: Found {len(matches)} matches in core memory.")
    return matches
    pass
#endregion
"""

# Function to update the conversation history with new input
def update_conversation_history(history, user_input, model_response):
    """
    Update the conversation history with the latest user input and model response.
    :param history: The current conversation history as a tuple of (user_input, model_response).
    :param user_input: The latest user input as a string.
    :param model_response: The latest model response as a string.
    :return: A new tuple representing the updated conversation history.
    """
    # Ensure that both the user_input and model_response are strings
    user_input_str = user_input if isinstance(user_input, str) else ""
    print("SYSTEM MESSAGE: User input string:", user_input_str)
    model_response_str = model_response if isinstance(model_response, str) else ""
    print("SYSTEM MESSAGE: Model response string:", model_response_str)    
    # Return a new tuple with the updated history
    return (user_input_str, model_response_str)


conversation_history = deque([], maxlen=20)

def summarize_user_input(user_input, startup_context):
    global conversation_history
    print("SYSTEM MESSAGE: Processing user input...")
  
    # Now update the global conversation history
    conversation_history = update_conversation_history(user_input, startup_context, conversation_history)
    print("SYSTEM MESSAGE: Conversation history updated successfully!")
  # Update conversation history with every input and pass context
    # Example of summarization logic placeholder
    summary_prompt_template = """# MISSION
    You are a Sparse Priming Representation (SPR) writer. An SPR is a particular kind of use of language for advanced NLP, NLU, and NLG tasks, particularly useful for the latest generation of Large Language Models (LLMs). You will be given information by the USER which you are to render as an SPR.

    # THEORY
    LLMs are a kind of deep neural network. They have been demonstrated to embed knowledge, abilities, and concepts, ranging from reasoning to planning, and even to theory of mind. These are called latent abilities and latent content, collectively referred to as latent space. The latent space of an LLM can be activated with the correct series of words as inputs, which will create a useful internal state of the neural network. This is not unlike how the right shorthand cues can prime a human mind to think in a certain way. Like human minds, LLMs are associative, meaning you only need to use the correct associations to "prime" another model to think in the same way.

    # METHODOLOGY
    Render the input as a distilled list of succinct statements, assertions, associations, concepts, analogies, and metaphors. The idea is to capture as much, conceptually, as possible but with as few words as possible. Write it in a way that makes sense to you, as the future audience will be another language model, not a human.
    The input to render is as follows: {conversation_history} """
    print("SYSTEM MESSAGE: LLM initialized successfully!")
    summary_prompt = PromptTemplate(input_variables=["conversation_history"], template=summary_prompt_template)
    print("SYSTEM MESSAGE: Prompt template created successfully!")
    summary_llm_chain = LLMChain(prompt=summary_prompt, llm=llm)
    print("SYSTEM MESSAGE: LLMChain created successfully!")
    summary = summary_llm_chain(conversation_history)  # Placeholder for actual summary
    print("SYSTEM MESSAGE: Checking for memory cues...")
    unique_id = add_to_long_term_memory(summary)  # Add summary to long-term memory and get its ID
    print("SYSTEM MESSAGE: Summary added to long-term memory = uniqueID.")
    if user_input.startswith("REMEMBERTHIS!:"):
        new_fact_entry = user_input.split("REMEMBERTHIS!:")[1].strip()
        add_to_core_memory(new_fact_entry, unique_id)
        print("SYSTEM MESSAGE: Fact added to core memory.")
        # Add to core memory with link to long-term memory ID
    elif user_input.startswith("REMEMBERTHAT!"):
        if 'conversation_history' in context and context['conversation_history']:
            new_fact_entry = context['conversation_history'][-2]['user_input']  # Get the statement before REMEMBERTHAT!
            add_to_core_memory(new_fact_entry, unique_id)  # Add to core memory with link to long-term memory ID
    elif user_input.startswith("CANYOUREMEMBER?:"):
        query = user_input.split("CANYOUREMEMBER?:")[1].strip()
        matches = search_core_memory(query)  # Search core memory for the query
        if matches:
            print("Here's what I remember:", matches)
            # Potentially retrieve and display related long-term memory context here
        else:
            print("I couldn't find any matching memory.")
    else:
        model_response, conversation_history = handle_conversational_input(user_input, conversation_history)
        print("SYSTEM MESSAGE: No memory cues found.")
        # Handle non-memory related input

def handle_conversational_input(user_input, history):
    if not isinstance(history, deque) or len(history) < 2:
        raise ValueError("History must be a deque with at least two elements.")

    # Retrieve the last two elements for context
    previous_user_input, previous_model_response = list(history)[-2:]

    # Check if previous_model_response is a dictionary and extract text, else use it as is
    previous_model_response_text = previous_model_response.get("text", "") if isinstance(previous_model_response, dict) else previous_model_response

    # Construct the prompt for the model
    prompt_for_model = str(previous_model_response_text) + " " + str(user_input) # Ensure it's a string concatenation

    # Get the model's response
    model_response_dict = llm_chain(prompt_for_model)
    model_response_text = model_response_dict.get("text", "") if isinstance(model_response_dict, dict) else model_response_dict

    # Update the conversation history
    history.append((user_input, model_response_text))  # Add the new pair to the history
    if len(history) > 20:  # Assuming we want to keep the last 20 interactions
        history.popleft()  # Remove the oldest element if history exceeds 20

    # Return the model's response and the updated history
    return model_response_text, history
#region ARCHIVAL MEMORY######################################


# Function to create and initialize a DeepLake dataset if it does not exist
def initialize_deeplake_dataset(dataset_path, embeddings):
    # Check if the dataset already exists
    if not os.path.exists(dataset_path):
        # If the dataset does not exist, create it and add embeddings
        db = DeepLake(dataset_path="histories/Wes_and_Finn_11-5-23_OurFirstConvo", embedding=embeddings, overwrite=True)
        print(f"SYSTEM MESSAGE: Created a new DeepLake dataset at {dataset_path}")
    else:
        # If the dataset exists, just open it without overwriting
        db = DeepLake(dataset_path=dataset_path, embedding=embeddings, overwrite=False)
        print(f"SYSTEM MESSAGE: Opened existing DeepLake dataset at {dataset_path}")
    return db


def add_documents_to_deeplake(db, docs, embeddings):
    # Assuming docs is a list of strings and embeddings is a numpy array with the corresponding embeddings
    db.add_documents(docs)


def run_cli_loop():
    global conversation_history  # Reference the global variable

    # Ensure conversation_history is a deque with maxlen 5
    if not isinstance(conversation_history, deque):
        conversation_history = deque(maxlen=5)  # Fallback initialization if needed
    elif len(conversation_history) < 2:
        conversation_history.extend([""] * (2 - len(conversation_history)))  # Add empty strings if needed

    while True:
        try:
            user_input = input('\033[1;96m' + 'USER> ' + '\033[0m')  # Bold Medium Light Blue
            if "FINN> SOMETHINGABOUTME!" in user_input or "FINN> UPDATEYOURIDENTITY!" in user_input:
                handle_identity_input(user_input)
            elif any(cue in user_input for cue in ["FINN> REMEMBERTHIS!", "FINN> REMEMBERTHAT!", "FINN> CANYOUREMEMBER?"]):
                summarize_user_input(user_input)
            elif user_input.lower() == 'exit':
                print('\033[3;37m' + "SYSTEM MESSAGE: Exiting the program." + '\033[0m')  # Light Grey, Italic
                break
            else:
                model_response, conversation_history = handle_conversational_input(user_input, history=conversation_history)
                print('\033[1;3;92m' + "FINN: " + model_response + '\033[0m')  # Bold, Italic, Light Pastel Green
        except KeyboardInterrupt:
            print('\033[3;37m' + "SYSTEM MESSAGE: \nExiting FINN..." + '\033[0m')  # Light Grey, Italic
            break
        


def main():
    global conversation_history
    conversation_history = deque(["", ""], maxlen=5)
    startup_context = initial_startup_context_informer()
    print("SYSTEM MESSAGE: Startup context:", startup_context)
    initial_model_response = process_input(user_input=startup_context)
    print("SYSTEM MESSAGE: Sending startup context to model...")
    
    run_cli_loop()
if __name__ == "__main__":
    main()  # This will call the main function when the script is executed
